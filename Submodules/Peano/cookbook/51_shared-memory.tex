\section{Shared memory parallelisation}
  \label{section:parallelisation:shared-memory}


\chapterDescription
  {
    30 minutes.
  }
  {
    A working simulation code and a compiler that supports either OpenMP or
    Intel's Threading Building Blocks (TBB).
  }

In this section, we discuss how to parallelise a simulation code on a shared
memory architecture.
Hereby, we focus on Peano's parallelisation features. 
In mature, big applications, they are typically supplemented by further
parallelisation that is application-specific: Peano can run lots of routines in
parallel if it is correctly used. 
This way, we are able to exploit several cores.
To exploit modern multicore and manycore architectures, codes however also have
to use parallelised routines, linear algebra, and so forth.
This is an additional level of parallelism that cannot be tackled here.

\subsection{Preparation}
\label{section:shared-memory:preparation}

Peano compiles in parallel out-of-the-box if you use the PDT to setup a project
blueprint. 
To facilitate a shared memory parallel build, you have to translate your code
with the compile flag \texttt{-DSharedTBB}. 
Alternative variants are \texttt{-DSharedOMP}, e.g.
Once you edit your makefile and add this compile flag, please also provide the
correct include and link paths to the makefile.
For modern Intel compilers, no changes should be required, as Intel's TBB come
along with the compiler suite.

By default, the auto-generated \texttt{main} configures the parallel environment. 
While OpenMP relies on the setup of a well-suited thread count via environment
variables, TBB requires/allows the user to select a thread count manually. 
If you want to configure the thread count this way, please add the corresponding
instructions to your \texttt{main}.
To make your code portable (and to preserve a serial version), I recommend to
embed all shared memory-specific routines into ifdefs.
Besides the aforementioned \texttt{SharedXXX} defines, Peano also provides a
flag \texttt{SharedMemoryParallelisation} that is set as soon as OpenMP or TBB
is selected.
To make use of it, you have to include
\linebreak \texttt{MulticoreDefinitions.h}.

\begin{code}
#ifdef SharedMemoryParallelisation
#include "tarch/multicore/Core.h"
#endif

#include "tarch/multicore/MulticoreDefinitions.h"

  // should be generated by the PDT
  int sharedMemorySetup = peano::initSharedMemoryEnvironment();
  ...


  // manual configuration of threads (optional)
  #ifdef SharedMemoryParallelisation
  const int         numberOfCores    = 16;
  tarch::multicore::Core::getInstance().configure(numberOfCores);
  #endif

\end{code}

\noindent
Peano's kernel uses multiple threads in several places. 
However, most of these concurrent fragments are really very short-running.
As a result, it is not clear whether it pays off to use multithreading or not.
Therefore Peano uses an oracle---an object that returns per grid
traversal phase whether multitasking should be used or not.
This oracle can be found in \linebreak
\texttt{peano/datatraversal/autotuning}\footnote{The namespace choice
\texttt{autotuning} is wrong. The namespace collects classes that allow you to
tailor your shared memory parallelisation. Our original design intention has
always been to work with autotuning to get rid of as many parameters as
possible. That's where the namespace identifier comes from.}.
Details on this oracle are discussed later.
For the time being, insert a commands into your runner.

\label{page:shared_memory:dummy-oracle}
\begin{code}
int mynamespace::runners::Runner::run() {
  #ifdef SharedMemoryParallelisation
  peano::datatraversal::autotuning::Oracle::getInstance().setOracle(
    new peano::datatraversal::autotuning::OracleForOnePhaseDummy(true)
  );
  #endif
  ...
}
\end{code}

\noindent
The \texttt{true} parameters enables multicore support.
It might be reasonable to study the other parameters (see either the header
file or Peano's webpages) later.
Right now, it should be possible to recompile the code and to run a first
version on a shared memory machine.
It probably won't yield that much parallel speedup though \ldots


\subsection{Specifying concurrency levels}

Peano's fundamental idea is that users use events to say what is to be done. 
But codes leave it to the kernel to decide when and---anticipating some
constraints---in which order it is done.
This property is exploited by the multicore variant: 
Peano mappings specify whether multiple calls to one event (such as
\texttt{enterCell}) may run in parallel.
The kernel then decides autonomously whether to run in parallel and on which
cores.


To make this work, we have to revise the concept of an adapter. 
An adapter invokes multiple events.
Therefore, the concurrency level of an event has to be the most pessimistic
combination of the concurrency levels of all mappings realising this event.
This combination is automatically determined by the adapters generated by the
PDT.


Concurrency levels are specified within the events. 
Per event there is one concurrency specification. 
\texttt{touchVertexFirstTime}'s concurrency level for example is specified by
the routine \linebreak
\texttt{touchVertexFirstTimeSpecification}.
If you want to run \texttt{touchVertexFirstTime} in parallel, open all mappings
and edit \texttt{touchVertexFirstTimeSpecification} in each individual one.


A specification returns an instance of \texttt{peano::MappingSpecification}. 
Such an instance accepts parameters:
\begin{itemize}
  \item The first flag specifies whether the corresponding event works on the
  whole tree, only on its leaves, or whether it actually does not implement
  anything at all.
  \item The second flag specifies whether a particular event may run in
  parallel.
  \item The third flag specify whether a particular event modifies the mapping's
  state itself. It basically means whether the event can be considered by Peano
  as \texttt{const}. If an event does not change the mapping's state, we
  obviously can parallelise more aggressively and do not have to merge back any
  changes in the mapping with \texttt{mergeWithWorkerThread}.
  \item Further flags specify whether events support resiliency and other
  experimental features. For most Peano kernel variants, such further flags are
  not supported. We maintain these variants in experimental Peano kernels only.
\end{itemize}

\begin{remark}
Even if you run your code without any shared memory parallelisation, it makes
sense to tailor all specifications. If your code does work on the finest tree
levels only, e.g., you can obtain significant speedup if you change the
\texttt{WholeTree} flag into \texttt{OnlyLeaves}. The most significant (serial)
speedups are obtained if you mark all specs as \texttt{Nop} where the actual
mapping does not do anything in the corresponding events. In this case, the
Peano kernel can skip whole function calls completely.
\end{remark}

\begin{remark}
If you tune/parallelise particular events and if you, at the same time, use
predefined mappings, you may have to study the specification objects
constructed there. Adapters always have to work pessimistically. If a predefined
mapping requires a particular event to be called sequentially (for plotters,
e.g.), you can specify any concurrency level you want---the kernel always will
run the whole event serially.
\end{remark}


For a quick start, I recommend to pick one particular event that is not
empty and where you do know exactly that it can run in parallel with other
events.
Select the correct concurrency level then:
\begin{itemize}
  \item Serial specifies that this event may not run in parallel and
  \item All other variants allow the kernel to issue events in parallel.
  However, they anticipate certain data dependencies---you may for example
  decide that entering a cell may be issued in parallel as long as no two events
  can access two adjacent vertices at the same time. This way, you anticipate
  data races.
\end{itemize}



\subsection{Ensuring inter-thread data consistency}

Once a parallel event is identified, the Peano kernel may run it on multiple
threads in parallel.
For this, the whole mapping object is replicated. 
The replication triggers the mapping's copy constructor.
Once the parallel phase is processed---all cells have been entered in parallel,
e.g.---all mapping replica are destroyed again and merged into one mapping that
is held by the adapter.
The mappings provide routines to plug into this life cycle.

In the copy constructor, you have to copy all mapping attributes that you need
in your parallel routines.
Two scenarios appear most often: Globally read properties such as a state object
are copied to each thread instance of the mapping.
Globally written attributes such as a global residual are set to zero in the
copy constructor:

\begin{code}
#if defined(SharedMemoryParallelisation)
mynamespace::MyMapping::MyMapping(const Collision&  masterThread):
  _localState( masterThread._localState ) {
  // alternatively, we could call
  // _localState = masterThread._localState;
  
  // now we clear all reduced/accumulated data:
  _localState.clearAttributes();
}
\end{code}

\noindent
The counterpart of the copy constructor is the routine
\texttt{mergeWithWorkerThread} that is invoked every time a mapping has been
replicated to run in parallel on multiple cores and this parallel phase is about
to terminate.
Peano does not use the destructor of the mapping to merge data to obtain a finer
control of thread replica and to be able to reuse mapping instances.

Usually, the merger only reduces globally accumulated data in this routine. 
If you have followed the recommendation in Section
\ref{section:applications:heat-equation} to make mappings hold copies of the
\texttt{State} object and to offer a merge routine, the mapping's shared
memory code resembles

\begin{code}
void mynamespace::MyMapping::::mergeWithWorkerThread(
  const mynamespace::MyMapping& workerThread
) {
  logTraceIn( "mergeWithWorkerThread(Collision)" );

  _localState.merge( workerThread._localState );
  
  logTraceOut( "mergeWithWorkerThread(Collision)" );
}
#endif

\end{code}


\subsection{Tailoring the oracle}

The oracle is the central point of control to decide whether event should be
invoked in parallel for given problem sizes.
The mapping specifications decide whether events may run in parallel.
The oracle specifies whether concurrent events should run in parallel for a
given problem size.


Whenever the kernel runs into a particular set of events and decides that it
would like to invoke those events in parallel, it realises the following
workflow:
\begin{itemize}
  \item Ask the adapter whether its combination of events allows the kernel to
  run events in parallel. If the result is a yes:
  \item Tell the oracle which adapter currently is active.
  \item Pass the oracle the problem size and ask which grain size (minimal
  problem chunk size) might be used. 
  \item If the adapter returns 0, nothing is ran in parallel. 0 should be
  returned by the oracle if the problem overall is too small to benefit from any
  shared memory parallelisation.
  \item Otherwise, split the problem into sizes of size at least grain size,
  replicate the mapping as often as required, and start using multiple threads.
\end{itemize}

\begin{remark}
 To make Peano as lightweight as possible, it internally works with static
 partitioning only. Features such as dynamic subpartitioning of problems are
 switched off. This lightweight approach in turn means that choosing a proper
 grain size for each and every step  is important. The oracles allow you to make
 exactly this choice.
\end{remark}


\noindent
As clarified, each adapter is associated to one oracle, i.e.~you are able to
use oracles specifying grain and minimal problem sizes on a per-adapter base.
Furthermore, oracles are not static.
They have a state and thus can for example `learn' which grain sizes are
reasonable (cmp.~paper by Nogina et al.).
For a quick start, trial and error with
\texttt{peano::datatraversal::autotuning::}
\texttt{OracleForOnePhaseDummy} is usually sufficient: 
Here you can hardcode grain sizes via the oracle's constructor.
Just modify some of the default values and study how the actual CPU usage
and the time-to-solution change.
Once you have detailed knowledge about your application's behaviour, it might
be reasonable to replace the Dummy with a tailored implementation of
\texttt{peano::datatraversal::autotuning::OracleForOnePhase}.


Peano's homepage offers a few more sophisticated oracles. 
Notably, there is one oracle that samples all different types of grain sizes and
one oracle that does real-time measurements to ``learn'' what good grain sizes
look like. 
These oracles come along with some Phyton scripts that allows users to analyse
their insights graphically or as tables.
See the class documentations.


Important for all sophisticated oracles is that they give the user the
opportunity to plot their statistics within the runner. 
To obtain these statistics, call
\begin{code}
  peano::datatraversal::autotuning::Oracle::getInstance().plotStatistics( "" );
\end{code}
The routine expects an filename.
If none is given, i.e.~you pass the empty string, all output is written to the
terminal.
Many oracles also provide routines to reload these statistics.
This way, statistical data is not always created from scratch but incrementally
updated.


\begin{remark}
 Under the umbrella of the ExaHyPE project, Peano's autotuning oracle named
 \texttt{OracleForOnePhaseWithShrinkingGrainSize} saw some significant
 improvement. It uses real-time measurements to identify good grain sizes. 
 If real-time measurements are supported on your system---we faced some issues
 on KNL systems where the timers seem to give imprecise answers---then this
 oracle usually does a good job. While the oracle realises ideas published in
 the paper by Nogina et al.~it saw some significant improvements since the
 publication. Please consult the extensive class documentation in the header for
 details.
\end{remark}


\noindent
Peano's dummy oracle as used on page \pageref{page:shared_memory:dummy-oracle}
works with a lot of default parameters.
They are chosen empirically as they seem to work reasonably on most codes.
However, these ``most'' codes all codes that use Peano under
the hood for algorithms which have their own memory management (they load in
patches per AMR grid cell for example).
If you have a code that has requires Peano to deliver more memory throughput,
ensure you select the \texttt{Split} strategy and uses pipelining: 

\begin{code}
new peano::datatraversal::autotuning::OracleForOnePhaseDummy(
 true,  //   bool useMultithreading                  = true,
 0,     //   int  grainSizeOfUserDefinedRegions      = 0,
 peano::datatraversal::autotuning::OracleForOnePhaseDummy::SplitVertexReadsOnRegularSubtree::Split,
 true, //  bool pipelineDescendProcessing
 true, //   bool pipelineAscendProcessing
 ...
\end{code}





\subsection{Working with Peano's tasks, semaphores, locks and loops}
\label{section:51_shared-memory:tasks}


All shared memory parallelisation standards today provide tasks, semaphores,
locks, and so forth.
Peano provides a wrapper around those which allows you to create one
implementation that then runs with OpenMP, TBBs and so forth.
To use them, I recommend to study all classes stored in
\texttt{tarch::multicore}.


For Peano applications that leave it to Peano to parallelise the code, 
i.e.~that decide to specify all concurrency through the specification 
objects, notably \texttt{BooleanSemaphore} and \texttt{Lock} are of interest:

\begin{code}
  static tarch::multicore::BooleanSemaphore  mysemaphore;
  
  
  tarch::multicore::Lock   myLock(mysemaphore);
  
  ...            // critical section
  
  myLock.free(); // destructor would free semaphore automatically
\end{code}


\noindent
The code above is removed automatically if you do not use OpenMP or TBB.
Otherwise, it introduces a critical section that is processed by at most one
thread at a time.
This way, you can let Peano run events concurrently, but still make them 
exchange/maintain shared data.


Furthermore, the header \texttt{Loop.h} might be of interest. 
It provides very useful macros:
\begin{itemize}
  \item \texttt{pfor} is a macro resembling a simple for-loop. If you compile
  your code with any shared memory compile flag, it makes the loop run in
  parallel.
  \item \texttt{pdfor} is a \texttt{pfor} as well. However, it does not traverse
  a linear sequence but runs over a $d$-dimensional index set. Very useful
  within a cell, e.g., where you have to do something with all vertices, while
  all vertices can be processed in parallel.
\end{itemize}


Peano's task concept basically mirrors TBB's task concept. 
However, if you implement everything within Peano's context, you can also switch
to OpenMP later on.
The idea is very simple: You create a new functor (a class 
with an operation \texttt{operator()} that does you job and
you create an instance of this class in your code.

\begin{code}
  void MyTaskClass::operator()() {
   ...   // do whatever you wanna do in your task 
  } 
\end{code}

\noindent
This class then is to be passed over to the \texttt{TaskSet} class:

\begin{code}
  MyTaskClass myTask(...); // pass in the constructor whatever data your task needs 
  peano::datatraversal::TaskSet( myTask, flag ); // the task now runs in the background
\end{code}

\noindent
\texttt{flag} is from the enum \texttt{peano::datatraversal::TaskSet::TaskType}.
The type \texttt{TaskSet} has multiple constructors. 
Those accepting multiple constructors run these constructors in parallel. 
For the constructor with only argument, you typically make \texttt{flag} specify that 
the passed task is a background task, i.e.~you want Peano to continue immediately 
and the task system to process your functor at any time later. 
Please note that such functors have to copy all data they require.
Please note that lambda expressions might be a useful alternative here to avoid
the overhead involved with writing dedicated task classes.



This single-functor constructor does not wait for the task to finish.
A classic pattern is that you introduce a boolean set to false before you launch
the task.
Modifications to the boolean are protected by a semaphore.
Once the task's \texttt{operator()} function finishes, it sets the bool.
The main thread meanwhile continues, but then there's a while loop that
continues to poll the boolean

\begin{code}
  bool terminated = false;
  while (!terminated) {
    tarch::multicore::Lock myLock( _myStaticSemphore );
    terminated = _myGlobalBool;
  }
\end{code}

\noindent
until it is set. Alternatively, there's an operation in \texttt{tarch::multicore::jobs} which 
allows you to tell Peano to wait until all background tasks have terminated.





\subsection{Profiling}

Peano provides some tailored, built-in profiling for shared memory codes. 
See the Section \ref{section:performance-analysis} for some details on this. 
The idea is that you recompile your code with an additional flag.
This gives you some output (pipe it into a file), and then there is a Python
script in the directory \texttt{peano/performanceanalysis} that allows you to
create a HTML page from this output that comprises lots of performance analysis
graphs and gives you hints where your CPUh are burnt. Obviously, this approach
is problematic for large runs producing lots of data, and it also suffers from
quite an overhead polluting your actual results.

If you want to use Intel's VTune Amplifier, we see many analyses be polluted,
too, as we use tons of templates, tons of inlinings consequently, and thus mess
up many profiles. To get more verbose outputs with the correct meta data, please
recompile your code with \texttt{-debug inline-debug-info}.


\subsection*{Further reading}

\begin{itemize}
  \item Weinzierl, Tobias, Bader, Michael, Unterweger, Kristof and Wittmann,
  Roland (2014). {\em Block Fusion on Dynamically Adaptive Spacetree Grids for
  Shallow Water Waves}. Parallel Processing Letters 24(3): 1441006.
  \item Schreiber, Martin, Weinzierl, Tobias and Bungartz, Hans-Joachim (2013).
  {\em Cluster Optimization and Parallelization of Simulations with Dynamically
  Adaptive Grids}. Euro-Par 2013, Berlin Heidelberg, Springer-Verlag.
  \item Schreiber, Martin, Weinzierl, Tobias and Bungartz, Hans-Joachim (2013).
  {\em SFC-based Communication Metadata Encoding for Adaptive Mesh}. Proceedings
  of the International Conference on Parallel Computing (ParCo), IOS Press.
  \item Nogina, Svetlana, Unterweger, Kristof and Weinzierl, Tobias (2012).
  {\em Autotuning of Adaptive Mesh Refinement PDE Solvers on Shared Memory
  Architectures}. PPAM 2011, Heidelberg, Berlin, Springer-Verlag.
  \item Eckhardt, Wolfgang and Weinzierl, Tobias (2010). {\em A Blocking
  Strategy on Multicore Architectures for Dynamically Adaptive PDE Solvers}.
  Parallel Processing and Applied Mathematics, PPAM 2009, Springer-Verlag.
\end{itemize}
