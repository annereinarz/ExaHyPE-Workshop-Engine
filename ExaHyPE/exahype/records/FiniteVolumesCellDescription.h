#ifndef _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class FiniteVolumesCellDescription;
      class FiniteVolumesCellDescriptionPacked;
   }
}

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   18/12/2018 23:45
 */
class exahype::records::FiniteVolumesCellDescription { 
   
   public:
      
      typedef exahype::records::FiniteVolumesCellDescriptionPacked Packed;
      
      enum CompressionState {
         Uncompressed = 0, CurrentlyProcessed = 1, Compressed = 2
      };
      
      enum RefinementEvent {
         None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
      };
      
      enum Type {
         Erased = 0, Ancestor = 1, Cell = 2, Descendant = 3
      };
      
      struct PersistentRecords {
         int _solverNumber;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
         #endif
         bool _hasCompletedLastStep;
         double _timeStepSize;
         double _timeStamp;
         double _previousTimeStepSize;
         double _previousTimeStamp;
         int _solutionIndex;
         int _solutionAveragesIndex;
         int _solutionCompressedIndex;
         void* _solution;
         void* _solutionAverages;
         void* _solutionCompressed;
         int _previousSolutionIndex;
         int _previousSolutionAveragesIndex;
         int _previousSolutionCompressedIndex;
         void* _previousSolution;
         void* _previousSolutionAverages;
         void* _previousSolutionCompressed;
         int _extrapolatedSolutionIndex;
         int _extrapolatedSolutionAveragesIndex;
         int _extrapolatedSolutionCompressedIndex;
         void* _extrapolatedSolution;
         void* _extrapolatedSolutionAverages;
         void* _extrapolatedSolutionCompressed;
         CompressionState _compressionState;
         int _bytesPerDoFInPreviousSolution;
         int _bytesPerDoFInSolution;
         int _bytesPerDoFInExtrapolatedSolution;
         int _level;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _offset;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _size;
         #endif
         Type _type;
         int _parentIndex;
         RefinementEvent _refinementEvent;
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const double& previousTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& extrapolatedSolutionIndex, const int& extrapolatedSolutionAveragesIndex, const int& extrapolatedSolutionCompressedIndex, void* extrapolatedSolution, void* extrapolatedSolutionAverages, void* extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _hasCompletedLastStep;
         }
         
         
         
         inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _hasCompletedLastStep = hasCompletedLastStep;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStamp = timeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getExtrapolatedSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionIndex(const int& extrapolatedSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionIndex = extrapolatedSolutionIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionAveragesIndex(const int& extrapolatedSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionAveragesIndex = extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressedIndex(const int& extrapolatedSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionCompressedIndex = extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolution;
         }
         
         
         
         inline void setExtrapolatedSolution(void* extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolution = extrapolatedSolution;
         }
         
         
         
         inline void* getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionAverages;
         }
         
         
         
         inline void setExtrapolatedSolutionAverages(void* extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
         }
         
         
         
         inline void* getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionCompressed;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressed(void* extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _compressionState;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _compressionState = compressionState;
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInSolution;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInSolution = bytesPerDoFInSolution;
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _bytesPerDoFInExtrapolatedSolution;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _type = type;
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _parentIndex = parentIndex;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementEvent = refinementEvent;
         }
         
         
         
      };
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         FiniteVolumesCellDescription();
         
         /**
          * Generated
          */
         FiniteVolumesCellDescription(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         FiniteVolumesCellDescription(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const double& previousTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& extrapolatedSolutionIndex, const int& extrapolatedSolutionAveragesIndex, const int& extrapolatedSolutionCompressedIndex, void* extrapolatedSolution, void* extrapolatedSolutionAverages, void* extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
         
         /**
          * Generated
          */
         ~FiniteVolumesCellDescription();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._neighbourMergePerformed[elementIndex];
            
         }
         
         
         
         inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
            
         }
         
         
         
         inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasCompletedLastStep;
         }
         
         
         
         inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasCompletedLastStep = hasCompletedLastStep;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStamp = timeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getExtrapolatedSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionIndex(const int& extrapolatedSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionIndex = extrapolatedSolutionIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionAveragesIndex(const int& extrapolatedSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionAveragesIndex = extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressedIndex(const int& extrapolatedSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionCompressedIndex = extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolution;
         }
         
         
         
         inline void setExtrapolatedSolution(void* extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolution = extrapolatedSolution;
         }
         
         
         
         inline void* getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionAverages;
         }
         
         
         
         inline void setExtrapolatedSolutionAverages(void* extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
         }
         
         
         
         inline void* getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionCompressed;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressed(void* extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._compressionState;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._compressionState = compressionState;
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInPreviousSolution = bytesPerDoFInPreviousSolution;
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInSolution;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInSolution = bytesPerDoFInSolution;
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._bytesPerDoFInExtrapolatedSolution;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._bytesPerDoFInExtrapolatedSolution = bytesPerDoFInExtrapolatedSolution;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._type = type;
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementEvent = refinementEvent;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const CompressionState& param);
         
         /**
          * Generated
          */
         static std::string getCompressionStateMapping();
         
         /**
          * Generated
          */
         static std::string toString(const RefinementEvent& param);
         
         /**
          * Generated
          */
         static std::string getRefinementEventMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         FiniteVolumesCellDescriptionPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
   
};

#ifndef DaStGenPackedPadding
  #define DaStGenPackedPadding 1      // 32 bit version
  // #define DaStGenPackedPadding 2   // 64 bit version
#endif


#ifdef PackedRecords
   #pragma pack (push, DaStGenPackedPadding)
#endif

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   18/12/2018 23:45
 */
class exahype::records::FiniteVolumesCellDescriptionPacked { 
   
   public:
      
      typedef exahype::records::FiniteVolumesCellDescription::CompressionState CompressionState;
      
      typedef exahype::records::FiniteVolumesCellDescription::Type Type;
      
      typedef exahype::records::FiniteVolumesCellDescription::RefinementEvent RefinementEvent;
      
      struct PersistentRecords {
         int _solverNumber;
         tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> _neighbourMergePerformed;
         double _timeStepSize;
         double _timeStamp;
         double _previousTimeStepSize;
         double _previousTimeStamp;
         int _solutionIndex;
         int _solutionAveragesIndex;
         int _solutionCompressedIndex;
         void* _solution;
         void* _solutionAverages;
         void* _solutionCompressed;
         int _previousSolutionIndex;
         int _previousSolutionAveragesIndex;
         int _previousSolutionCompressedIndex;
         void* _previousSolution;
         void* _previousSolutionAverages;
         void* _previousSolutionCompressed;
         int _extrapolatedSolutionIndex;
         int _extrapolatedSolutionAveragesIndex;
         int _extrapolatedSolutionCompressedIndex;
         void* _extrapolatedSolution;
         void* _extrapolatedSolutionAverages;
         void* _extrapolatedSolutionCompressed;
         int _level;
         tarch::la::Vector<DIMENSIONS,double> _offset;
         tarch::la::Vector<DIMENSIONS,double> _size;
         Type _type;
         int _parentIndex;
         RefinementEvent _refinementEvent;
         
         /** mapping of records:
         || Member 	|| startbit 	|| length
          |  hasCompletedLastStep	| startbit 0	| #bits 1
          |  compressionState	| startbit 1	| #bits 2
          |  bytesPerDoFInPreviousSolution	| startbit 3	| #bits 3
          |  bytesPerDoFInSolution	| startbit 6	| #bits 3
          |  bytesPerDoFInExtrapolatedSolution	| startbit 9	| #bits 3
          */
         int _packedRecords0;
         
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const double& previousTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& extrapolatedSolutionIndex, const int& extrapolatedSolutionAveragesIndex, const int& extrapolatedSolutionCompressedIndex, void* extrapolatedSolution, void* extrapolatedSolutionAverages, void* extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
         }
         
         
         
         inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _timeStamp = timeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getExtrapolatedSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionIndex(const int& extrapolatedSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionIndex = extrapolatedSolutionIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionAveragesIndex(const int& extrapolatedSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionAveragesIndex = extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressedIndex(const int& extrapolatedSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionCompressedIndex = extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolution;
         }
         
         
         
         inline void setExtrapolatedSolution(void* extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolution = extrapolatedSolution;
         }
         
         
         
         inline void* getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionAverages;
         }
         
         
         
         inline void setExtrapolatedSolutionAverages(void* extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionAverages = extrapolatedSolutionAverages;
         }
         
         
         
         inline void* getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedSolutionCompressed;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressed(void* extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(compressionState) << (1));
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (3));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (3));
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (6));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (6));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (6));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (6));
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (9));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (9));
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _type = type;
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _parentIndex = parentIndex;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _refinementEvent = refinementEvent;
         }
         
         
         
      };
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         FiniteVolumesCellDescriptionPacked();
         
         /**
          * Generated
          */
         FiniteVolumesCellDescriptionPacked(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         FiniteVolumesCellDescriptionPacked(const int& solverNumber, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed, const bool& hasCompletedLastStep, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const double& previousTimeStamp, const int& solutionIndex, const int& solutionAveragesIndex, const int& solutionCompressedIndex, void* solution, void* solutionAverages, void* solutionCompressed, const int& previousSolutionIndex, const int& previousSolutionAveragesIndex, const int& previousSolutionCompressedIndex, void* previousSolution, void* previousSolutionAverages, void* previousSolutionCompressed, const int& extrapolatedSolutionIndex, const int& extrapolatedSolutionAveragesIndex, const int& extrapolatedSolutionCompressedIndex, void* extrapolatedSolution, void* extrapolatedSolutionAverages, void* extrapolatedSolutionCompressed, const CompressionState& compressionState, const int& bytesPerDoFInPreviousSolution, const int& bytesPerDoFInSolution, const int& bytesPerDoFInExtrapolatedSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
         
         /**
          * Generated
          */
         ~FiniteVolumesCellDescriptionPacked();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char> getNeighbourMergePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._neighbourMergePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setNeighbourMergePerformed(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,signed char>& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._neighbourMergePerformed = (neighbourMergePerformed);
         }
         
         
         
         inline signed char getNeighbourMergePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._neighbourMergePerformed[elementIndex];
            
         }
         
         
         
         inline void setNeighbourMergePerformed(int elementIndex, const signed char& neighbourMergePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._neighbourMergePerformed[elementIndex]= neighbourMergePerformed;
            
         }
         
         
         
         inline bool getHasCompletedLastStep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
         }
         
         
         
         inline void setHasCompletedLastStep(const bool& hasCompletedLastStep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( hasCompletedLastStep ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStamp = timeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline double getPreviousTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStamp;
         }
         
         
         
         inline void setPreviousTimeStamp(const double& previousTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStamp = previousTimeStamp;
         }
         
         
         
         inline int getSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionIndex;
         }
         
         
         
         inline void setSolutionIndex(const int& solutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionIndex = solutionIndex;
         }
         
         
         
         inline int getSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAveragesIndex;
         }
         
         
         
         inline void setSolutionAveragesIndex(const int& solutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAveragesIndex = solutionAveragesIndex;
         }
         
         
         
         inline int getSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressedIndex;
         }
         
         
         
         inline void setSolutionCompressedIndex(const int& solutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressedIndex = solutionCompressedIndex;
         }
         
         
         
         inline void* getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(void* solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline void* getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(void* solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAverages = solutionAverages;
         }
         
         
         
         inline void* getSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionCompressed;
         }
         
         
         
         inline void setSolutionCompressed(void* solutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionCompressed = solutionCompressed;
         }
         
         
         
         inline int getPreviousSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionIndex;
         }
         
         
         
         inline void setPreviousSolutionIndex(const int& previousSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionIndex = previousSolutionIndex;
         }
         
         
         
         inline int getPreviousSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAveragesIndex;
         }
         
         
         
         inline void setPreviousSolutionAveragesIndex(const int& previousSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAveragesIndex = previousSolutionAveragesIndex;
         }
         
         
         
         inline int getPreviousSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressedIndex;
         }
         
         
         
         inline void setPreviousSolutionCompressedIndex(const int& previousSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressedIndex = previousSolutionCompressedIndex;
         }
         
         
         
         inline void* getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolution;
         }
         
         
         
         inline void setPreviousSolution(void* previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolution = previousSolution;
         }
         
         
         
         inline void* getPreviousSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionAverages;
         }
         
         
         
         inline void setPreviousSolutionAverages(void* previousSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionAverages = previousSolutionAverages;
         }
         
         
         
         inline void* getPreviousSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolutionCompressed;
         }
         
         
         
         inline void setPreviousSolutionCompressed(void* previousSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolutionCompressed = previousSolutionCompressed;
         }
         
         
         
         inline int getExtrapolatedSolutionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionIndex(const int& extrapolatedSolutionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionIndex = extrapolatedSolutionIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionAveragesIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionAveragesIndex(const int& extrapolatedSolutionAveragesIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionAveragesIndex = extrapolatedSolutionAveragesIndex;
         }
         
         
         
         inline int getExtrapolatedSolutionCompressedIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressedIndex(const int& extrapolatedSolutionCompressedIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionCompressedIndex = extrapolatedSolutionCompressedIndex;
         }
         
         
         
         inline void* getExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolution;
         }
         
         
         
         inline void setExtrapolatedSolution(void* extrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolution = extrapolatedSolution;
         }
         
         
         
         inline void* getExtrapolatedSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionAverages;
         }
         
         
         
         inline void setExtrapolatedSolutionAverages(void* extrapolatedSolutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionAverages = extrapolatedSolutionAverages;
         }
         
         
         
         inline void* getExtrapolatedSolutionCompressed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedSolutionCompressed;
         }
         
         
         
         inline void setExtrapolatedSolutionCompressed(void* extrapolatedSolutionCompressed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedSolutionCompressed = extrapolatedSolutionCompressed;
         }
         
         
         
         inline CompressionState getCompressionState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (CompressionState) tmp;
         }
         
         
         
         inline void setCompressionState(const CompressionState& compressionState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((compressionState >= 0 && compressionState <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(compressionState) << (1));
         }
         
         
         
         inline int getBytesPerDoFInPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (3));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (3));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInPreviousSolution(const int& bytesPerDoFInPreviousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInPreviousSolution >= 1 && bytesPerDoFInPreviousSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (3));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInPreviousSolution) - 1) << (3));
         }
         
         
         
         inline int getBytesPerDoFInSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (6));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (6));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInSolution(const int& bytesPerDoFInSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInSolution >= 1 && bytesPerDoFInSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (6));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInSolution) - 1) << (6));
         }
         
         
         
         inline int getBytesPerDoFInExtrapolatedSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   tmp = tmp + 1;
   assertion(( tmp >= 1 &&  tmp <= 7));
   return (int) tmp;
         }
         
         
         
         inline void setBytesPerDoFInExtrapolatedSolution(const int& bytesPerDoFInExtrapolatedSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion((bytesPerDoFInExtrapolatedSolution >= 1 && bytesPerDoFInExtrapolatedSolution <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (9));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | (static_cast<int>(bytesPerDoFInExtrapolatedSolution) - 1) << (9));
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._type = type;
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementEvent = refinementEvent;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const CompressionState& param);
         
         /**
          * Generated
          */
         static std::string getCompressionStateMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         static std::string toString(const RefinementEvent& param);
         
         /**
          * Generated
          */
         static std::string getRefinementEventMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         FiniteVolumesCellDescription convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
   
};

#ifdef PackedRecords
#pragma pack (pop)
#endif


#endif

