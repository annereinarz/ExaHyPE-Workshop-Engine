#ifndef _EXAHYPE_RECORDS_VERTEX_H
#define _EXAHYPE_RECORDS_VERTEX_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>
#include "peano/utils/Globals.h"

namespace exahype {
   namespace records {
      class Vertex;
      class VertexPacked;
   }
}

#if defined(Parallel) && defined(PersistentRegularSubtrees) && defined(Asserts)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   15/11/2018 18:02
    */
   class exahype::records::Vertex { 
      
      public:
         
         typedef exahype::records::VertexPacked Packed;
         
         enum InsideOutsideDomain {
            Inside = 0, Boundary = 1, Outside = 2
         };
         
         enum RefinementControl {
            Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6, EnforceRefinementTriggered = 7
         };
         
         struct PersistentRecords {
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
            #endif
            bool _isHangingNode;
            RefinementControl _refinementControl;
            int _adjacentCellsHeight;
            InsideOutsideDomain _insideOutsideDomain;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _x;
            #endif
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
            #endif
            bool _adjacentSubtreeForksIntoOtherRank;
            bool _parentRegularPersistentSubgrid;
            bool _parentRegularPersistentSubgridInPreviousIteration;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _CellDescriptionsIndex;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _CellDescriptionsIndex = (CellDescriptionsIndex);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _ADERDGCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _ADERDGCellDescriptions = (ADERDGCellDescriptions);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _FiniteVolumesCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isHangingNode;
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isHangingNode = isHangingNode;
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementControl;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementControl = refinementControl;
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _insideOutsideDomain;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _insideOutsideDomain = insideOutsideDomain;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _x = (x);
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
            inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentRegularPersistentSubgrid;
            }
            
            
            
            inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
            }
            
            
            
            inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentRegularPersistentSubgridInPreviousIteration;
            }
            
            
            
            inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            int _adjacentCellsHeightOfPreviousIteration;
            int _numberOfAdjacentRefinedCells;
            
         public:
            /**
             * Generated
             */
            Vertex();
            
            /**
             * Generated
             */
            Vertex(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            /**
             * Generated
             */
            Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            /**
             * Generated
             */
            virtual ~Vertex();
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._CellDescriptionsIndex;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
            }
            
            
            
            inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._CellDescriptionsIndex[elementIndex];
               
            }
            
            
            
            inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._ADERDGCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
            }
            
            
            
            inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._ADERDGCellDescriptions[elementIndex];
               
            }
            
            
            
            inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._FiniteVolumesCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
            }
            
            
            
            inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
               
            }
            
            
            
            inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
               
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._isHangingNode;
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._isHangingNode = isHangingNode;
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._refinementControl;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._refinementControl = refinementControl;
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfAdjacentRefinedCells;
            }
            
            
            
            inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._insideOutsideDomain;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._insideOutsideDomain = insideOutsideDomain;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._x = (x);
            }
            
            
            
            inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._x[elementIndex];
               
            }
            
            
            
            inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._x[elementIndex]= x;
               
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._adjacentRanks[elementIndex];
               
            }
            
            
            
            inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
               
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
            inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentRegularPersistentSubgrid;
            }
            
            
            
            inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
            }
            
            
            
            inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._parentRegularPersistentSubgridInPreviousIteration;
            }
            
            
            
            inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const InsideOutsideDomain& param);
            
            /**
             * Generated
             */
            static std::string getInsideOutsideDomainMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementControl& param);
            
            /**
             * Generated
             */
            static std::string getRefinementControlMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            VertexPacked convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               #endif
      
   };
   
   #ifndef DaStGenPackedPadding
     #define DaStGenPackedPadding 1      // 32 bit version
     // #define DaStGenPackedPadding 2   // 64 bit version
   #endif
   
   
   #ifdef PackedRecords
      #pragma pack (push, DaStGenPackedPadding)
   #endif
   
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   15/11/2018 18:02
    */
   class exahype::records::VertexPacked { 
      
      public:
         
         typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
         
         typedef exahype::records::Vertex::RefinementControl RefinementControl;
         
         struct PersistentRecords {
            tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
            tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
            tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
            int _adjacentCellsHeight;
            tarch::la::Vector<DIMENSIONS,double> _x;
            int _level;
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
            
            /** mapping of records:
            || Member 	|| startbit 	|| length
             |  isHangingNode	| startbit 0	| #bits 1
             |  refinementControl	| startbit 1	| #bits 3
             |  insideOutsideDomain	| startbit 4	| #bits 2
             |  adjacentSubtreeForksIntoOtherRank	| startbit 6	| #bits 1
             |  parentRegularPersistentSubgrid	| startbit 7	| #bits 1
             |  parentRegularPersistentSubgridInPreviousIteration	| startbit 8	| #bits 1
             */
            int _packedRecords0;
            
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _CellDescriptionsIndex;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _CellDescriptionsIndex = (CellDescriptionsIndex);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _ADERDGCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _ADERDGCellDescriptions = (ADERDGCellDescriptions);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _FiniteVolumesCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _x = (x);
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
            }
            
            
            
            inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (7);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (7);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
            }
            
            
            
            inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (8);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (8);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
            }
            
            
            
         };
         private: 
            PersistentRecords _persistentRecords;
            int _adjacentCellsHeightOfPreviousIteration;
            int _numberOfAdjacentRefinedCells;
            
         public:
            /**
             * Generated
             */
            VertexPacked();
            
            /**
             * Generated
             */
            VertexPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            /**
             * Generated
             */
            VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
            
            /**
             * Generated
             */
            virtual ~VertexPacked();
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._CellDescriptionsIndex;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
            }
            
            
            
            inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._CellDescriptionsIndex[elementIndex];
               
            }
            
            
            
            inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._ADERDGCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
            }
            
            
            
            inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._ADERDGCellDescriptions[elementIndex];
               
            }
            
            
            
            inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._FiniteVolumesCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
            }
            
            
            
            inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
               
            }
            
            
            
            inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
               
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfAdjacentRefinedCells;
            }
            
            
            
            inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._x = (x);
            }
            
            
            
            inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._x[elementIndex];
               
            }
            
            
            
            inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._x[elementIndex]= x;
               
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._adjacentRanks[elementIndex];
               
            }
            
            
            
            inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
               
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (7);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (7);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (8);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (8);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const InsideOutsideDomain& param);
            
            /**
             * Generated
             */
            static std::string getInsideOutsideDomainMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementControl& param);
            
            /**
             * Generated
             */
            static std::string getRefinementControlMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            Vertex convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               #endif
      
   };
   
   #ifdef PackedRecords
   #pragma pack (pop)
   #endif
   
   
   #elif defined(PersistentRegularSubtrees) && defined(Asserts) && !defined(Parallel)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _x;
               #endif
               int _level;
               bool _parentRegularPersistentSubgrid;
               bool _parentRegularPersistentSubgridInPreviousIteration;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<DIMENSIONS,double> _x;
               int _level;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                |  parentRegularPersistentSubgrid	| startbit 6	| #bits 1
                |  parentRegularPersistentSubgridInPreviousIteration	| startbit 7	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif defined(Parallel) && !defined(PersistentRegularSubtrees) && defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6, EnforceRefinementTriggered = 7
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _x;
               #endif
               int _level;
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               #endif
               bool _adjacentSubtreeForksIntoOtherRank;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<DIMENSIONS,double> _x;
               int _level;
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                |  adjacentSubtreeForksIntoOtherRank	| startbit 6	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif defined(Parallel) && defined(PersistentRegularSubtrees) && !defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6, EnforceRefinementTriggered = 7
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               #endif
               bool _adjacentSubtreeForksIntoOtherRank;
               bool _parentRegularPersistentSubgrid;
               bool _parentRegularPersistentSubgridInPreviousIteration;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                |  adjacentSubtreeForksIntoOtherRank	| startbit 6	| #bits 1
                |  parentRegularPersistentSubgrid	| startbit 7	| #bits 1
                |  parentRegularPersistentSubgridInPreviousIteration	| startbit 8	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (8);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (8);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (8);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (8);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif defined(PersistentRegularSubtrees) && !defined(Parallel) && !defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               bool _parentRegularPersistentSubgrid;
               bool _parentRegularPersistentSubgridInPreviousIteration;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgrid;
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgrid = parentRegularPersistentSubgrid;
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._parentRegularPersistentSubgridInPreviousIteration = parentRegularPersistentSubgridInPreviousIteration;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                |  parentRegularPersistentSubgrid	| startbit 6	| #bits 1
                |  parentRegularPersistentSubgridInPreviousIteration	| startbit 7	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const bool& parentRegularPersistentSubgrid, const bool& parentRegularPersistentSubgridInPreviousIteration);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               inline bool getParentRegularPersistentSubgrid() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgrid(const bool& parentRegularPersistentSubgrid) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgrid ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline bool getParentRegularPersistentSubgridInPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setParentRegularPersistentSubgridInPreviousIteration(const bool& parentRegularPersistentSubgridInPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (7);
   _persistentRecords._packedRecords0 = static_cast<int>( parentRegularPersistentSubgridInPreviousIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif defined(Parallel) && !defined(PersistentRegularSubtrees) && !defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6, EnforceRefinementTriggered = 7
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               #endif
               bool _adjacentSubtreeForksIntoOtherRank;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                |  adjacentSubtreeForksIntoOtherRank	| startbit 6	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentRanks[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                  
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (6);
   _persistentRecords._packedRecords0 = static_cast<int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif !defined(PersistentRegularSubtrees) && defined(Asserts) && !defined(Parallel)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               #ifdef UseManualAlignment
               tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<DIMENSIONS,double> _x;
               #endif
               int _level;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<DIMENSIONS,double> _x;
               int _level;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._x = (x);
               }
               
               
               
               inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._x[elementIndex];
                  
               }
               
               
               
               inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._x[elementIndex]= x;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   #elif !defined(PersistentRegularSubtrees) && !defined(Parallel) && !defined(Asserts)
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::Vertex { 
         
         public:
            
            typedef exahype::records::VertexPacked Packed;
            
            enum InsideOutsideDomain {
               Inside = 0, Boundary = 1, Outside = 2
            };
            
            enum RefinementControl {
               Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5
            };
            
            struct PersistentRecords {
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               #endif
               #ifdef UseManualAlignment
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions __attribute__((aligned(VectorisationAlignment)));
               #else
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               #endif
               bool _isHangingNode;
               RefinementControl _refinementControl;
               int _adjacentCellsHeight;
               InsideOutsideDomain _insideOutsideDomain;
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _insideOutsideDomain = insideOutsideDomain;
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   15/11/2018 18:02
       */
      class exahype::records::VertexPacked { 
         
         public:
            
            typedef exahype::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef exahype::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _CellDescriptionsIndex;
               tarch::la::Vector<TWO_POWER_D,void*> _ADERDGCellDescriptions;
               tarch::la::Vector<TWO_POWER_D,void*> _FiniteVolumesCellDescriptions;
               int _adjacentCellsHeight;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               
            };
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               VertexPacked();
               
               /**
                * Generated
                */
               VertexPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex, tarch::la::Vector<TWO_POWER_D,void*> ADERDGCellDescriptions, tarch::la::Vector<TWO_POWER_D,void*> FiniteVolumesCellDescriptions, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               virtual ~VertexPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getCellDescriptionsIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._CellDescriptionsIndex;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setCellDescriptionsIndex(const tarch::la::Vector<TWO_POWER_D,int>& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._CellDescriptionsIndex = (CellDescriptionsIndex);
               }
               
               
               
               inline int getCellDescriptionsIndex(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._CellDescriptionsIndex[elementIndex];
                  
               }
               
               
               
               inline void setCellDescriptionsIndex(int elementIndex, const int& CellDescriptionsIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._CellDescriptionsIndex[elementIndex]= CellDescriptionsIndex;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getADERDGCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._ADERDGCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setADERDGCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& ADERDGCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._ADERDGCellDescriptions = (ADERDGCellDescriptions);
               }
               
               
               
               inline void* getADERDGCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._ADERDGCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setADERDGCellDescriptions(int elementIndex, void*& ADERDGCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._ADERDGCellDescriptions[elementIndex]= ADERDGCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,void*> getFiniteVolumesCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._FiniteVolumesCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFiniteVolumesCellDescriptions(const tarch::la::Vector<TWO_POWER_D,void*>& FiniteVolumesCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._FiniteVolumesCellDescriptions = (FiniteVolumesCellDescriptions);
               }
               
               
               
               inline void* getFiniteVolumesCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._FiniteVolumesCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setFiniteVolumesCellDescriptions(int elementIndex, void*& FiniteVolumesCellDescriptions)
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._FiniteVolumesCellDescriptions[elementIndex]= FiniteVolumesCellDescriptions;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 5));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 5));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(refinementControl) << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   int mask =  (1 << (2)) - 1;
   mask = static_cast<int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | static_cast<int>(insideOutsideDomain) << (4));
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               Vertex convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  enum class ExchangeMode { Blocking, NonblockingWithPollingLoopOverTests, LoopOverProbeWithBlockingReceive };
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, ExchangeMode mode );
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  #endif
         
      };
      
      #ifdef PackedRecords
      #pragma pack (pop)
      #endif
      
      
      
   
#endif

#endif

